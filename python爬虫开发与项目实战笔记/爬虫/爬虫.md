#、爬虫的工作流程
	1、首先选取一部分精心挑选的种子URL
	2、将这些URL放入待抓取URL队列
	3、从待抓取URL队列中读取待抓取队列的URL，解析DNS，并得到主机的IP，并将URL对应的网页下载下来，存储进
	已下载的网页库中。此外，将这些URL放进已抓取URL队列。
	4、分析已抓取URL队列中的URL，从已下载的网页数据中分析出其它URL，并和已抓取的URL进行比较去重，最后将
	去重过的URL放入待抓取URL队列，从而进入下一个循环。
	
 - 2.1.5 请求
	 - 分为 4部分内容:请求方法( Request Method )、 请求的网址 ( Request URL )、请求头( Request Headers )、 请求体( Request Body )。
	 - 1、请求方法：
		- 1.1、GET：
		- 1.2、POST：
		- 区别：
			- GET请求中的参数包含在 URL里面，数据可以在 URL中看到，而 POST请求的 URL不会包 含这些数据，数据都是通过表单形式传输的，会包含在请求体中 。
			- GET请求提交的数据最多只有 1024字节，而 POST方式没有限制 。
	 - 2、请求头
		- Accept：请求报头域，用于指定客户端可接受哪些类型的信息
		- Accept-Language：指定客户端可接受的语言类型
		- Accept-Encoding：指定客户端可接受的内容编码
		- Host：用于指定请求资源的主机IP和端口号，其内容为请求 URL 的原始服务器或网关的位置。
		- Cookie：这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。维持当前访问会话。
		- Referer：此内容用来标识请求是从哪里来的，服务器可以拿到这一信息并做相 应的处理，如做来源统计、防盗链处理等 。
		- User-Agent：使服务器识别客户使用的操作系统及版本 、 浏览器及版本等信息 。 在做爬虫时加上此信息，可以伪装为浏览器;如果不加，很可能会被识别州为爬虫 。
		- Content-Type：也叫互联网媒体类型( Internet Media Type )或者 MIME类型，在 HTTP协议消息头中，它用来表示具体请求中的媒体类型信息 。
     - 3、高级用法
        - Handler工具：可以理解为各种处理器，有专门处理验证登录的，有处理Cookies的，有代理设置的
        - urllib.request模块里的BaseHandler类，是所有其它Handler的父类，提供了最基本的方法。如default_open(),protocol_request()
        - 常用Handler子类：
            - HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误会抛出HTTPError类型异常
            - HTTPRedirectHandler：用于处理重定向
            - HTTPCookieProcessor：用于处理Cookies
            - ProxyHandler：用于设置代理，默认代理为空
            - HTTPPasswordMgr：用于管理密码，它维护了用户名和密码表
            - HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题
        - 另一个比较重要的类就是 OpenerDirector，我们可以称为 Opener。 我们之前用过 urlopen()这个 方法，实际上它就是 urllib为我们提供的一个 Opener       
#、requests库
    - 使用内容见requests内容文件夹
#、正则表达式
    - match():从字符串的起始位置匹配正则表达式。
    - search():，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果.
    - findall():会搜索整 个字符串，然后返回匹配正则表达式的所有内容 。
        - 如果只是获取第 一 个内容，可以用 search()方法。 当需要提取多个内容时，可以用于 indall()方法。
    - sub():
    - compile():可以将正则字符串编译成正 则表达式对象，以便在后面的匹配中复用 








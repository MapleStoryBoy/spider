## 分类算法-朴素贝叶斯算法
- 朴素贝叶斯使用场景：特征独立
- 朴素贝叶斯公式
- ![朴素贝叶斯公式](/Users/mac/Desktop/spider/机器学习/机器学习概述/朴素贝叶斯公式.jpeg)

- 公式分为三部分：
	- P(C):每个文档类别的概率（某文档类别数/总文档数量）
	- P(W/C):给定类别下特征（被预测文档中出现的词）的概率
		- 计算方法：P(F1|C) = Ni/N  (训练文档中去计算)
			- Ni为该F1词在C类别所有文档中出现的次数
			- N为所属类别C下的文档所有词出现的次数和
	- P(F1,F2,...)  预测文档中每个词的概率

- 拉普拉斯平滑系数
	-  解决词频列表里很多出现次数为0的现象。
	-  公式：P(F1|C) = (Ni+a)/(N+am)
		- a为指定的系数一般为1，m为训练文档中统计出的特征词个数

### 联合概率和条件概率
- 联合概率：包含多个条件，且所有条件同时成立的概率
	- 记作：P(A,B): P(A,B)=P(A)P(B)

- 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率
	- 记作：P(A|B)
	- 特性：P(A1,A2|B)=P(A1|B)P(A2|B)
	- 注意：此条件概率的成立，是由于A1，A2相互独立的结果

### sklearn朴素贝叶斯实现API
- sklearn.naive_bayes.MultinomialNB
- MultinomialNB函数
	- sklearn.naive_bayes.MultinomialNB(alpha=1.0)
		- 朴素贝叶斯分类
		- alpha：拉普拉斯平滑系数

- 注意：
	- 训练集误差大的化，它的结果肯定不好。
	- 不需要调参

- 朴素贝叶斯分类的优缺点
	- 优点：
		- 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
		- 对缺失数据不太敏感，算法也比较简单，常用于文本分类
		- 分类准确度高，速度快
	- 缺点：
		- 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

		
## 分类模型的评估
- estimator.score()
	- 一般最常见使用的是准确率，即预测结果正确的百分比

### 混淆矩阵
- 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

- 引出评估标准
	- 准确率
	- 精确率和召回率
- 分类模型评估API
- sklearn.metrics.classification_report(y_true,y_pred,target_names=None)
	- y_true:真实的目标值
	- y_pred:估计器预测目标值
	- target_names:目标类别名称
	- return:每个类别精确率与召回率

## 模型的选择与调优
- 1，交叉验证
	- 所有数据分成n等分，求平均模型结果
	- ![2](/Users/mac/Desktop/spider/机器学习/机器学习概述/2.jpeg)
	- 目的：为了让被评估的模型更加准确可信
- 2，超参数搜索-网格搜索
	- 调参数----K-近邻
	- 通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的k值），这种叫超参数。但是手动过程很复杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型
	
	- 超参数搜索-网格搜索API
	- sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
		- 对估计器的指定参数值进行详尽搜索
		- estimator：估计器对象
		- param_grid:估计器参数（dict）{"n_neighbors":[1,3,5]}
		- cv:指定几折交叉验证
		- fit：输入训练数据
		- score：准确率
		- 结果分析：
			- best_score_:在交叉验证中验证的最好结果
			- best_estimator_:最好的参数模型
			- cv_results_:每次交叉验证后的测试集准确率结果和训练集准确率结果

